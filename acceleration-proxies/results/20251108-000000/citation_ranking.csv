paperId,title,total_citations,a,b,R2,lead_author_id,last_author_id,citation_dates,citation_counts,lead_author_h_index,last_author_h_index,rank
34471a2fa18ea22efad5287cf4aeb18542c98a9b,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,4570,-276.79999999999995,100.43076923076926,0.7584154991365656,2307073216,2329999671,"[""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01""]","[0, 0, 0, 0, 0, 0, 0, 0, 311, 954, 999, 1000, 1000, 1000]",3.0,4.0,1
dd4cfde3e135f799a9a71b4f57e13a29de89f7e3,DAPO: An Open-Source LLM Reinforcement Learning System at Scale,770,93.11428571428571,63.43296703296704,0.8675937566539489,23716915,2285763682,"[""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01""]","[21, 54, 181, 265, 329, 422, 569, 741, 749, 749, 749, 749, 749, 749]",9.0,5.0,2
668075792a7ab40457d92e09da28d35c879271c3,Kimi k1.5: Scaling Reinforcement Learning with LLMs,601,40.828571428571415,50.91648351648353,0.914092496127803,2341539673,2341579297,"[""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01""]","[10, 36, 82, 139, 274, 361, 414, 457, 513, 583, 584, 584, 584, 584]",1.0,2.0,3
f2d0f3d47ae850f49a58f4977393bd0025af4bec,HybridFlow: A Flexible and Efficient RLHF Framework,714,-130.68571428571425,48.24835164835165,0.8119370039226881,2268673684,2318983809,"[""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01""]","[7, 10, 10, 12, 13, 20, 35, 68, 190, 268, 315, 400, 532, 681]",4.0,2.0,4
f82f49c20c6acc69f884f05e3a9f1ceea91061ce,Detecting hallucinations in large language models using semantic entropy,656,-38.8,27.276923076923083,0.9175877104553661,33859827,2303846295,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[16, 20, 23, 33, 43, 80, 102, 121, 138, 175, 222, 260, 324, 382]",23.0,6.0,5
ae736662f64d56f3ab1894fbd9c45f8f37251843,OpenAssistant Conversations - Democratizing Large Language Model Alignment,741,-17.257142857142842,24.2923076923077,0.9742681603357825,3996967,102888911,"[""2023-05-01"", ""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01""]","[8, 25, 40, 49, 63, 78, 119, 149, 162, 183, 226, 256, 288, 323]",8.0,3.0,6
c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb,Training Language Models to Self-Correct via Reinforcement Learning,267,-12.199999999999982,19.283516483516486,0.96257722316695,2317038858,2258552654,"[""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01""]","[12, 28, 28, 33, 41, 63, 86, 108, 148, 182, 188, 201, 222, 244]",4.0,5.0,7
600ff4c4ae9fc506c86673c5ecce4fa90803e987,RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,453,1.114285714285715,15.400000000000002,0.9850571130119078,2132475367,2188497,"[""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01""]","[9, 19, 31, 45, 51, 64, 95, 112, 128, 147, 164, 179, 185, 188]",5.0,20.0,8
182c7b40ff7560a5545764814338f55a2098e441,Reinforced Self-Training (ReST) for Language Modeling,350,-4.199999999999998,13.404395604395607,0.97934244588122,146372255,1737568,"[""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01""]","[4, 14, 23, 30, 37, 50, 74, 92, 111, 122, 143, 147, 155, 159]",15.0,76.0,9
9a75e23639bfcc3a51da57a3b682a984d1d8ac0b,Language Models can Solve Computer Tasks,435,-5.685714285714273,11.292307692307695,0.9837593339410006,2176466425,143953836,"[""2023-04-01"", ""2023-05-01"", ""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01""]","[2, 6, 21, 29, 37, 48, 56, 71, 83, 89, 100, 114, 140, 152]",3.0,19.0,10
1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0,Reinforcement Learning for Reasoning in Large Language Models with One Training Example,138,40.45714285714286,9.501098901098903,0.7829334981173772,2334890311,2324671790,"[""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01""]","[4, 33, 55, 74, 85, 102, 133, 135, 135, 135, 135, 135, 135, 135]",8.0,5.0,11
900cd128482bbab4d2752d01ce80c55498b78dd2,SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution,102,14.40000000000001,7.839560439560441,0.9009383971730187,2237736409,2324830843,"[""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01""]","[5, 13, 21, 42, 50, 54, 66, 83, 96, 97, 97, 97, 97, 97]",8.0,2.0,12
bde841b0dbbf7a15ee69966a828c7fe2cf532ad9,WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning,85,-10.813186813186809,7.648351648351649,0.9515219133522951,2286747770,2243402027,"[""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01""]","[1, 3, 4, 8, 11, 14, 33, 44, 49, 57, 67, 82, 83]",7.0,33.0,13
9bf00afb0efb02a263fa3ddea1e768677498536c,Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging,198,-9.457142857142856,6.938461538461539,0.9631655215767981,2000091730,2233163820,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[2, 2, 6, 6, 14, 17, 24, 35, 48, 54, 61, 65, 80, 85]",16.0,5.0,14
d318e0169f649656c71f02a1f84194a734fe1962,Reward Design with Language Models,264,-8.17142857142857,6.48791208791209,0.9639780795005277,37909625,1779671,"[""2023-03-01"", ""2023-04-01"", ""2023-05-01"", ""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01""]","[2, 2, 3, 9, 17, 20, 26, 34, 41, 46, 53, 61, 76, 86]",11.0,61.0,15
8fd11c6f3eb1d0aeb915369f3c4f0b1bb24cab0c,Large Language Model Unlearning,205,-11.000000000000002,6.472527472527474,0.9539450347540543,2257134957,2279671647,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[1, 2, 2, 3, 8, 15, 22, 29, 41, 48, 53, 58, 74, 79]",8.0,5.0,16
67f03ac399693393116076c0b8ec8ea05b910685,WARM: On the Benefits of Weight Averaged Reward Models,125,7.228571428571445,6.34945054945055,0.9820420063902049,2280134846,151047979,"[""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01""]","[6, 11, 18, 24, 32, 44, 48, 52, 59, 72, 74, 75, 78, 86]",9.0,17.0,17
69535d3c6ff3238f8e7b2b29c1d40ea9d9d7914f,Reasoning with Exploration: An Entropy Perspective,95,36.228571428571435,6.173626373626374,0.6082587092673627,2068324576,2253471545,"[""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01"", ""2026-08-01""]","[5, 15, 34, 65, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95]",8.0,15.0,18
1c66c41ff22adf66bb9b06d87d5a2ab7b8ee8de7,"Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities",141,-4.742857142857139,5.971428571428572,0.9188188180089802,2302316320,2302284624,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[4, 4, 9, 12, 15, 24, 31, 35, 37, 40, 47, 53, 77, 89]",6.0,2.0,19
131a13c60f179511572abc81d6bd6aa988e96854,Rule Based Rewards for Language Model Safety,84,11.912087912087907,5.835164835164836,0.9932715916736827,2319225702,2297773848,"[""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01""]","[14, 18, 21, 30, 34, 40, 47, 55, 59, 63, 69, 80, 80]",4.0,5.0,20
f7749635a5fc0492ef4705bee963ffa887bb2865,Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning,120,-6.2571428571428624,5.632967032967034,0.8704281601865684,119692515,2268967207,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[4, 6, 10, 11, 14, 17, 19, 25, 29, 32, 44, 56, 75, 83]",16.0,10.0,21
10feab31bb9e71a0f1094fb00c0554abfb992c4d,ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models,77,20.057142857142853,5.507692307692308,0.7172462470419474,2369203659,2359888593,"[""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01""]","[5, 12, 19, 28, 52, 74, 74, 74, 74, 74, 74, 74, 74, 74]",2.0,4.0,22
89e184d2bc830af568e439db9476caa0c047e11a,Guiding Pretraining in Reinforcement Learning with Large Language Models,216,-6.514285714285706,5.353846153846155,0.9693954363679135,144894286,2112400,"[""2023-03-01"", ""2023-04-01"", ""2023-05-01"", ""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01""]","[1, 3, 3, 8, 12, 16, 23, 27, 36, 38, 46, 51, 63, 69]",16.0,51.0,23
0c43750030198dbe7fe164e1ce743ec64427bca1,Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms,91,4.51648351648351,5.247252747252748,0.9749273100440415,2301519261,2791038,"[""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[9, 11, 13, 15, 29, 32, 34, 37, 47, 52, 53, 64, 72]",9.0,36.0,24
0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,221,-3.457142857142861,4.916483516483517,0.9782241062326449,2003745905,1720664,"[""2023-03-01"", ""2023-04-01"", ""2023-05-01"", ""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01""]","[1, 3, 6, 13, 17, 19, 21, 26, 32, 40, 47, 52, 57, 65]",4.0,54.0,25
8e3b1f5d8b6c165f64137cc1f7dea89cf6f622bd,d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning,57,7.399999999999999,4.795604395604396,0.806981111608293,2260172378,2267723293,"[""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01""]","[1, 6, 11, 16, 21, 37, 56, 56, 56, 56, 56, 56, 56, 56]",6.0,3.0,26
ac258100ebe178287ae4ae3dc7ac78f8c27e017d,Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game,120,-1.6571428571428561,4.793406593406595,0.983390064925198,2170477930,2108052575,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[4, 5, 5, 9, 15, 21, 26, 30, 38, 45, 48, 51, 56, 60]",6.0,8.0,27
7ae48b24cbf955bf9b9498fb287bf4c5cd3b73d4,OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning,129,0.05714285714286267,4.738461538461539,0.9835083829142581,2273620042,2111637832,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[5, 6, 8, 14, 20, 23, 24, 30, 36, 42, 47, 52, 60, 65]",9.0,15.0,28
4aef44e4aeaf28868ae2f1fff2c4eb19ff4df1f6,The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning,67,20.857142857142865,4.527472527472528,0.7096938195294573,1923351,2334095077,"[""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01""]","[6, 13, 21, 30, 49, 65, 65, 65, 65, 65, 65, 65, 65, 65]",13.0,5.0,29
c78350e81298ca87bc1d59b466fa40081232caaa,Teaching Large Language Models to Reason with Reinforcement Learning,134,3.8857142857142875,4.512087912087912,0.9849652208560354,2279337437,48647153,"[""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[6, 10, 13, 18, 21, 23, 27, 39, 40, 44, 46, 53, 59, 66]",6.0,27.0,30
8d6411e337502f7fe0bfa59d486803a73d2c1192,Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling,56,7.714285714285719,4.384615384615385,0.9257669803322274,2329057292,2328936204,"[""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01""]","[2, 8, 13, 19, 30, 36, 37, 41, 46, 55, 55, 55, 55, 55]",7.0,3.0,31
529ff7d6441d244212cf2becafd12a7e67ac56d9,FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,177,-0.8857142857142818,3.9604395604395606,0.9771728467135712,2162042348,2237499232,"[""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01""]","[4, 4, 7, 10, 14, 18, 21, 24, 27, 32, 41, 45, 47, 54]",9.0,10.0,32
04c05c6acc970f2ca89af8e436b8dd8189396146,General-Reasoner: Advancing LLM Reasoning Across All Domains,56,17.828571428571433,3.685714285714286,0.7291672986500835,2461713,2249847177,"[""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01""]","[7, 12, 19, 27, 34, 54, 54, 54, 54, 54, 54, 54, 54, 54]",23.0,20.0,33
a50d4fd8f584276c0fd8560255884edd57aa926e,Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue,175,-3.835164835164832,3.664835164835165,0.9431849018842964,2193154278,144539290,"[""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01""]","[1, 1, 5, 8, 10, 10, 14, 18, 22, 26, 37, 40, 44]",3.0,9.0,34
2d906cda427cb2c4a71069423312e57ba4cd5445,Reinforcement Learning Enhanced LLMs: A Survey,40,-2.3516483516483535,3.6098901098901104,0.9653726416360269,2109514219,2285877067,"[""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01""]","[1, 1, 4, 8, 9, 12, 18, 25, 28, 34, 37, 37, 37]",11.0,7.0,35
90df9c970aa98aa49bcf79e252ab26b1bb6889c5,Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance,81,-1.3428571428571374,3.547252747252748,0.9734008923166604,2255687520,2253826001,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[3, 4, 6, 8, 13, 14, 16, 19, 26, 31, 36, 38, 43, 47]",8.0,7.0,36
668858489bbec3ce45f7a84a6a557b329f9ec91a,ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL,111,-1.799999999999994,3.30989010989011,0.9538781175085166,2288585404,1488785534,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[3, 4, 4, 7, 11, 14, 16, 17, 23, 26, 30, 32, 42, 47]",10.0,41.0,37
550006bea81e4ccb67743dd1b82a70b86b48d93a,RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback,91,-2.3142857142857136,3.16923076923077,0.9353864147548357,2108111472,3407045,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[2, 4, 4, 6, 10, 13, 14, 15, 20, 25, 28, 32, 35, 48]",9.0,23.0,38
6c00f661c46391642208a5292f38b5a9e0e09cae,AutoWebGLM: A Large Language Model-based Web Navigating Agent,105,-1.4285714285714246,3.10989010989011,0.9043779712501553,2051311700,2260595820,"[""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[1, 3, 7, 8, 10, 13, 16, 18, 21, 22, 26, 32, 47]",10.0,19.0,39
53106a642a12b05753ebe9ffca62d8efb0670281,Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models,39,1.5999999999999985,3.1054945054945056,0.9697990766219226,1819830,2258552654,"[""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01""]","[2, 2, 6, 10, 15, 18, 22, 25, 27, 30, 37, 37, 37, 37]",28.0,5.0,40
fe13cc3650fd7df8f98d0596bfc13178af82c799,Curiosity-driven Red-teaming for Large Language Models,68,-0.6285714285714269,3.0417582417582425,0.9809771586602479,33317877,2257003971,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[3, 3, 4, 8, 12, 13, 17, 17, 23, 26, 31, 34, 38, 39]",12.0,14.0,41
fb0207353fe923efd26a4fedafde56cd8eda1173,Characterization of Large Language Model Development in the Datacenter,82,0.7428571428571452,2.907692307692308,0.9688820372589667,2150570711,2146333441,"[""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[4, 5, 5, 9, 11, 14, 18, 22, 24, 25, 27, 30, 39, 42]",10.0,12.0,42
c3e2bec83b9105b7925aa76c0f38b88d2e337b31,Motif: Intrinsic Motivation from Artificial Intelligence Feedback,82,-1.714285714285712,2.8571428571428577,0.9714541921984756,26389489,2249760477,"[""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01""]","[2, 3, 4, 5, 6, 12, 14, 17, 20, 25, 29, 30, 31, 38]",8.0,4.0,43
8e27e73e61d772686c7ff30f8bcafc167058b231,"Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges",37,3.1978021978021953,2.813186813186814,0.9495218777166039,2284694862,2238832044,"[""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01""]","[3, 4, 6, 11, 14, 20, 23, 24, 29, 31, 32, 32, 32]",5.0,4.0,44
0a42291d9543eabe33f6c14278333484071a707c,Offline Reinforcement Learning for LLM Multi-Step Reasoning,36,2.714285714285718,2.791208791208792,0.9571765990528528,2216204941,2336830631,"[""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01""]","[1, 2, 7, 12, 15, 18, 23, 24, 25, 29, 34, 34, 34, 34]",3.0,2.0,45
95d638e7705ec561382268405bc488df4c26c7f7,Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning,39,8.600000000000003,2.7208791208791214,0.8156613553873827,2232782972,2351085693,"[""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01""]","[2, 4, 12, 18, 23, 25, 32, 36, 36, 36, 36, 36, 36, 36]",3.0,3.0,46
0659e2a187f43c5fa86632f3003f8b0d13dac329,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,66,-1.314285714285714,2.7186813186813192,0.9792047559777688,2066420772,2266465257,"[""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[1, 1, 4, 8, 9, 9, 12, 18, 23, 24, 26, 28, 31, 35]",22.0,10.0,47
e1770838ec0667cad48729a81764ed9964d6a8e6,LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis,103,1.5714285714285714,2.692307692307693,0.9706343687844634,3400291,1746959,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[4, 6, 8, 10, 11, 13, 14, 19, 23, 25, 28, 30, 37, 39]",6.0,27.0,48
dd38755291d108ab86c68d1aac7485921bb8e647,LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions,55,2.5704709717042965e-15,2.5934065934065935,0.9541037975030492,2051640686,2241828177,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[2, 2, 6, 8, 10, 15, 16, 17, 17, 19, 25, 28, 32, 39]",9.0,9.0,49
7ef10b9b5bbce61573f647774004f95221efe665,AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning,43,-2.3714285714285706,2.551648351648352,0.9480750767542703,2249844970,2256784186,"[""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01""]","[1, 2, 3, 4, 7, 10, 13, 13, 15, 18, 20, 26, 32, 35]",9.0,13.0,50
e3116a5d5784392d9190ca35997f65a252291032,Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks,62,7.406593406593402,2.4065934065934074,0.9744803624763801,35904540,2256996419,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01""]","[8, 9, 12, 15, 18, 21, 22, 23, 26, 27, 30, 33, 40]",13.0,8.0,51
1bca0871e893568c5fd346c14c17691f71a8f65a,Value Augmented Sampling for Language Model Alignment and Personalization,40,-0.8285714285714261,2.402197802197803,0.9794412674988214,2301077050,2257003971,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[1, 2, 4, 6, 7, 13, 13, 15, 15, 20, 24, 27, 29, 31]",3.0,14.0,52
02d7673c89a94367edd7a2cf0ea4f5540fba3e42,DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning,29,3.8000000000000016,2.382417582417583,0.9011856737479867,2348886016,2284641156,"[""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01""]","[3, 3, 5, 10, 15, 16, 21, 23, 29, 29, 29, 29, 29, 29]",3.0,7.0,53
4f0e4a313a3f777b4b6aab4f364b9bc51a6aacc9,Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging,34,10.171428571428576,2.347252747252748,0.833716340671871,2346255376,2347282055,"[""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01""]","[5, 7, 15, 16, 20, 26, 30, 33, 34, 34, 34, 34, 34, 34]",4.0,4.0,54
aa89c6bf86486e180833037333555e3492b15c8e,MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning,26,-0.3142857142857136,2.3230769230769233,0.9242590823168335,2232975456,2347076258,"[""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01""]","[0, 1, 4, 4, 8, 9, 14, 19, 23, 25, 25, 25, 25, 25]",3.0,1.0,55
24d9d00b91f99dde3c9a0ea5c79e63f2ed26151c,Reinforcing General Reasoning without Verifiers,29,6.200000000000001,2.3208791208791215,0.7023063067602122,2257107372,2356268297,"[""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01""]","[1, 4, 5, 6, 21, 29, 29, 29, 29, 29, 29, 29, 29, 29]",6.0,5.0,56
13c0f33ccd88607fce4819135e404a988aa8aad4,KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion,63,1.1758241758241728,2.175824175824176,0.9321856572189462,2273826353,2243335442,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01""]","[5, 5, 5, 7, 9, 11, 12, 14, 16, 19, 23, 28, 31]",3.0,13.0,57
87912571f3df29464d3ccafae66f6e1eed581564,Offline Regularised Reinforcement Learning for Large Language Models Alignment,39,-0.6857142857142874,2.14945054945055,0.9213084436224932,16326904,1808897,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[2, 2, 3, 5, 8, 12, 12, 14, 14, 14, 18, 21, 29, 32]",14.0,41.0,58
b2991a4b2ecc9db0fbd9ca738022801b4e5ee001,CogBench: a large language model walks into a psychology lab,50,-0.7142857142857114,2.1428571428571432,0.9814777531709281,2215168951,2271334207,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[1, 2, 2, 5, 7, 11, 11, 13, 17, 21, 21, 22, 24, 28]",7.0,7.0,59
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling,97,-1.1142857142857139,2.0615384615384618,0.9660335097389051,103596213,145609073,"[""2023-02-01"", ""2023-03-01"", ""2023-04-01"", ""2023-05-01"", ""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01""]","[0, 0, 4, 4, 8, 10, 12, 13, 14, 16, 18, 20, 23, 30]",5.0,18.0,60
4ed96712afa0d0e82cddb3d669d4e9f60195aecb,CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent,32,0.34285714285714397,2.0461538461538464,0.9727029407968937,2301015154,2266407320,"[""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01""]","[2, 3, 5, 7, 7, 8, 10, 14, 18, 20, 22, 23, 26, 26]",6.0,9.0,61
57451ce18f3035fcadf64db38420434f9299b7f3,Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF,29,-2.956043956043954,2.005494505494506,0.9519879380323558,2305569663,2305484057,"[""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[0, 0, 0, 1, 2, 6, 10, 11, 13, 18, 18, 19, 20]",4.0,3.0,62
1f45c9d4d92c51a94c984eb4c2c6e027bbf78038,SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM,35,12.742857142857153,1.9516483516483518,0.7551946061906223,2356633495,2356584590,"[""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01""]","[4, 11, 15, 21, 23, 26, 32, 32, 32, 32, 32, 32, 32, 32]",2.0,2.0,63
78875987dc674fc556873df037cf114f04932e80,"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",71,0.9142857142857149,1.9472527472527477,0.9786759755641441,1454018677,2269994509,"[""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01""]","[1, 1, 4, 6, 10, 11, 13, 16, 18, 20, 20, 22, 24, 24]",21.0,7.0,64
82f59319e581cdfe16a97fe29bec6215ad818a81,Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions,32,13.228571428571435,1.8659340659340664,0.6163089064550746,2300236632,2309265357,"[""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01"", ""2026-08-01""]","[3, 7, 14, 22, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31]",2.0,8.0,65
d5c358380b2b2fb9c0e8be6212158e0a01dacb16,TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction,45,-2.9142857142857133,1.8659340659340664,0.9336740047020421,2261392830,2261689267,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[1, 1, 1, 1, 3, 5, 7, 7, 10, 14, 16, 17, 22, 24]",1.0,4.0,66
765051eee0fb1394b62555edb86ba1f7a00892fb,Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking,26,6.285714285714291,1.8571428571428574,0.8163037824180723,2352756200,2352034310,"[""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01""]","[1, 4, 8, 13, 17, 18, 21, 25, 25, 25, 25, 25, 25, 25]",6.0,6.0,67
debe978e02b664fb7254b6c7b58a74c09ce897e3,Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents,61,-2.057142857142856,1.7890109890109895,0.974176664314279,145843537,2257036129,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[0, 1, 2, 2, 4, 5, 7, 10, 12, 15, 16, 18, 21, 21]",28.0,25.0,68
59084df7203c6be33838ba3e3854eb9bda053ed2,Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint,39,1.7714285714285725,1.7824175824175827,0.9830743821417095,2256558402,2274218622,"[""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01""]","[2, 5, 6, 6, 7, 11, 12, 14, 15, 19, 21, 21, 23, 25]",11.0,16.0,69
d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855,"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models",114,-1.828571428571428,1.7648351648351652,0.9382520061986628,25841722,2114939672,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01""]","[1, 1, 3, 3, 5, 5, 6, 7, 12, 14, 15, 18, 22, 23]",11.0,14.0,70
a72955288e6468b8342eff6f8f0281ceaefc526a,SelfIE: Self-Interpretation of Large Language Model Embeddings,42,-6.291007049108521e-16,1.7362637362637368,0.9593863370841789,2259331717,7700460,"[""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[2, 2, 2, 6, 6, 7, 8, 12, 16, 17, 17, 21, 21, 21]",3.0,14.0,71
3646acd0dc49a00d38517abccfc3a54cb78bbadc,SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning,25,9.615384615384606,1.7307692307692315,0.660039113428944,2347120431,2272708319,"[""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01""]","[2, 6, 11, 17, 24, 25, 25, 25, 25, 25, 25, 25, 25]",2.0,5.0,72
2bdccbeffa0bae760f416a72ebe7a3951e230659,Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective,25,8.771428571428576,1.6945054945054947,0.627891943278143,2365396554,2295863002,"[""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01"", ""2026-08-01""]","[1, 5, 7, 14, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]",3.0,6.0,73
3b5e7f456043fa96dc4673dc11156834fd9a985e,Learning diverse attacks on large language models for robust red-teaming and safety tuning,36,-0.14285714285714315,1.5604395604395607,0.9856869109538149,1472875852,1383135665,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[0, 2, 3, 4, 5, 9, 10, 10, 11, 14, 16, 17, 18, 21]",13.0,17.0,74
9c9ca3a8320c0babd9fc331cc376ffff32fe1f67,Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment,25,3.9714285714285724,1.5428571428571431,0.9272994129158513,2257001403,2257314035,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[0, 4, 7, 10, 12, 14, 15, 16, 16, 19, 20, 20, 21, 22]",13.0,8.0,75
6b4da2023c3a11aea6e3ccb9ab13e594833c47eb,Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics,38,-2.857142857142859,1.4835164835164838,0.954858382320581,3427937,,"[""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01""]","[0, 0, 0, 0, 1, 3, 6, 7, 9, 10, 12, 13, 16, 18]",10.0,,76
e0bf76edd8e6b793b81c16a915ede48e377ebab6,Efficient Reinforcement Learning with Large Language Model Priors,20,-2.114285714285714,1.4461538461538466,0.9169857675364284,2262502320,2152812755,"[""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01""]","[1, 1, 1, 2, 2, 3, 4, 8, 8, 9, 12, 15, 18, 18]",4.0,9.0,77
c226a4acb42912054d498bcf771023b0ba2da001,Language Model Self-improvement by Reinforcement Learning Contemplation,70,-2.942857142857143,1.364835164835165,0.832693117408907,1432234123,2152850415,"[""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01""]","[0, 0, 0, 2, 3, 3, 3, 3, 4, 8, 9, 12, 16, 20]",8.0,14.0,78
7d6168fbd3ed72f9098573007f4b8c2ec9e576b9,Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning,48,-1.6857142857142848,1.3362637362637366,0.9491219061312519,2190751523,2257129989,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[0, 1, 1, 2, 4, 4, 4, 6, 8, 11, 11, 13, 15, 18]",13.0,24.0,79
a70f0f9b9b9dc7d5caadcb23a551ea4213727548,Large Language Model-based Human-Agent Collaboration for Complex Task Solving,35,-0.9714285714285716,1.3252747252747257,0.986071675548143,2163940136,2266834033,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[0, 0, 2, 3, 4, 6, 7, 7, 9, 11, 12, 13, 16, 17]",3.0,7.0,80
76eb8409fccfe15a4d7ef8a8bb5ad0eddd097bd4,The Inadequacy of Reinforcement Learning From Human Feedbackâ€”Radicalizing Large Language Models via Semantic Vulnerabilities,35,8.65714285714286,1.3164835164835165,0.984837713579908,11430146,2070517,"[""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01""]","[8, 9, 11, 13, 15, 15, 17, 19, 20, 20, 22, 23, 24, 25]",17.0,30.0,81
8d0df3168870fd17b36ecd5575e406feb5a5a1b5,M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions,32,0.028571428571431016,1.2483516483516486,0.9050777085788027,2303565983,2261895678,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[2, 2, 2, 3, 5, 7, 7, 8, 9, 10, 11, 12, 15, 21]",3.0,6.0,82
ea72a82163f2cc458ca1004767531735d646a810,Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation,21,-3.2285714285714286,1.2109890109890113,0.8484622371270873,2284692039,2284672059,"[""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01"", ""2025-08-01"", ""2025-09-01"", ""2025-10-01"", ""2025-11-01""]","[0, 0, 0, 0, 0, 1, 1, 3, 5, 8, 9, 9, 12, 17]",2.0,2.0,83
dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe,Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment,23,0.4285714285714292,1.186813186813187,0.9545008183306056,2257404641,1729969,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[1, 3, 3, 4, 5, 6, 7, 8, 8, 9, 13, 14, 16, 17]",7.0,71.0,84
cc1446a3ccb51bcf8f68827fe05c6841a963cfa3,AutoTutor meets Large Language Models: A Language Model Tutor with Rich Pedagogy and Guardrails,37,-1.8,1.1120879120879124,0.9523731587561375,2105636395,2790926,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[0, 0, 1, 1, 1, 3, 4, 6, 7, 7, 9, 10, 13, 14]",7.0,37.0,85
ff59b0fe9faa8835c18c9a017de1c6d6fcd64408,A Multi-Expert Large Language Model Architecture for Verilog Code Generation,23,-2.1714285714285713,1.0703296703296705,0.7889197505197505,1605994999,2282247356,"[""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01""]","[1, 1, 1, 1, 1, 1, 2, 2, 4, 6, 7, 12, 14, 14]",4.0,4.0,86
7f96bb27a8fca35b1f7d02ee319a64be04114809,LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments,24,-0.4857142857142858,1.052747252747253,0.9435609565521353,2265927745,2291077490,"[""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[1, 1, 2, 2, 4, 4, 5, 6, 8, 8, 9, 10, 14, 15]",5.0,3.0,87
23a6037147eff3b2a44ca79e2a8867b6b296429b,Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation,21,0.22857142857142967,1.0527472527472528,0.9375462253549903,2284221616,3387382,"[""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01""]","[1, 1, 2, 2, 3, 8, 8, 8, 8, 9, 10, 12, 12, 15]",3.0,23.0,88
f9db375824ca667897229f570ceb699ab05f1483,Large Language Models are Learnable Planners for Long-Term Recommendation,27,-0.25714285714285623,1.006593406593407,0.9570398759010859,2153422494,2275173839,"[""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01""]","[1, 1, 2, 2, 3, 3, 6, 7, 9, 9, 9, 10, 13, 13]",8.0,7.0,89
1414653547b72a23d2c0ca987d270fb236c9105a,On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification,24,16.085714285714293,0.8659340659340656,0.3489547273299466,2290907197,2366165580,"[""2025-09-01"", ""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01"", ""2026-08-01"", ""2026-09-01"", ""2026-10-01""]","[3, 13, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]",6.0,1.0,90
b2ade4fc5464f3c0bfd92e8cce375bfd5db20e03,Evolutionary Large Language Model for Automated Feature Transformation,25,0.6000000000000005,0.8527472527472529,0.8120832883806237,2298757061,2239058389,"[""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01"", ""2025-06-01"", ""2025-07-01""]","[2, 3, 3, 3, 4, 4, 5, 5, 5, 7, 8, 9, 12, 16]",7.0,10.0,91
8db1d30ac06dde4bd19d0a86241137ac2be21552,LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward,29,1.1428571428571443,0.8461538461538461,0.9741070548417836,2215168083,71756373,"[""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01""]","[2, 2, 2, 3, 5, 5, 6, 7, 8, 10, 10, 10, 11, 12]",5.0,15.0,92
60e6e3767c36bf9e16b58b7221c5712b4d3d5293,Large Language Models Are Semi-Parametric Reinforcement Learning Agents,32,-1.057142857142858,0.7120879120879123,0.9197126336078499,2118333,1736727,"[""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01""]","[0, 0, 1, 1, 2, 2, 2, 3, 4, 4, 6, 7, 8, 10]",12.0,52.0,93
3714ed902e79dad5dcc93c5d033c8222d044f3c8,Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,28,1.362637362637362,0.7087912087912089,0.8701631457854004,2108813860,2061625488,"[""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01""]","[3, 3, 3, 3, 3, 4, 4, 5, 7, 8, 10, 10, 10]",7.0,13.0,94
9e0da24b96eb4f3f77ccdfc485a4d7d7d21a4585,Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents,23,0.3714285714285721,0.6351648351648352,0.9042494451361447,,2267730082,"[""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01""]","[1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 9, 9, 9]",,1.0,95
4cbf53465e2af3aa4f0079167a0474fc713f3ce0,RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models,20,0.9428571428571441,0.4703296703296704,0.8986656200941915,2290943894,2211183718,"[""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01"", ""2024-12-01"", ""2025-01-01"", ""2025-02-01"", ""2025-03-01"", ""2025-04-01"", ""2025-05-01""]","[2, 2, 2, 2, 2, 2, 3, 4, 5, 6, 6, 6, 7, 7]",2.0,5.0,96
0fe8f2f55046ad0b8d6337f57a78466790923264,Outcome-based Exploration for LLM Reasoning,26,22.14285714285715,0.4285714285714285,0.19999999999999996,2379687376,2237802765,"[""2025-10-01"", ""2025-11-01"", ""2025-12-01"", ""2026-01-01"", ""2026-02-01"", ""2026-03-01"", ""2026-04-01"", ""2026-05-01"", ""2026-06-01"", ""2026-07-01"", ""2026-08-01"", ""2026-09-01"", ""2026-10-01"", ""2026-11-01""]","[11, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]",1.0,10.0,97
c3b0da01017870729a7a83b94e7787e5105cbc32,Learning to Rewrite Prompts for Personalized Text Generation,36,-0.7428571428571431,0.36703296703296706,0.8703073802465283,2249775049,2240516450,"[""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01"", ""2024-11-01""]","[0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 3, 3, 4, 5]",6.0,9.0,98
28c6ac721f54544162865f41c5692e70d61bccab,A survey on large language model based autonomous agents,1845,0.0,0.0,0.0,2152509786,153693432,"[""2023-09-01"", ""2023-10-01"", ""2023-11-01"", ""2023-12-01"", ""2024-01-01"", ""2024-02-01"", ""2024-03-01"", ""2024-04-01"", ""2024-05-01"", ""2024-06-01"", ""2024-07-01"", ""2024-08-01"", ""2024-09-01"", ""2024-10-01""]","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",6.0,54.0,99
