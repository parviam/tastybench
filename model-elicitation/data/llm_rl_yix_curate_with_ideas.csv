paper_id,abstract,title,idea
dd4cfde3e135f799a9a71b4f57e13a29de89f7e3,"Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",DAPO: An Open-Source LLM Reinforcement Learning System at Scale,"Decoupling the clipping operation from policy updates and applying dynamic sampling to adaptively select training data, DAPO optimizes large language models via reinforcement learning at scale. The algorithm integrates four novel techniques within the verl framework, enabling efficient, reproducible large‑scale RL training of LLMs."
668075792a7ab40457d92e09da28d35c879271c3,"Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",Kimi k1.5: Scaling Reinforcement Learning with LLMs,"Training a multimodal large language model with reinforcement learning by leveraging long‑context scaling and enhanced policy optimization, while deliberately omitting complex components such as Monte‑Carlo tree search, explicit value functions, and separate reward models. Incorporating a “long‑to‑short” pipeline that distills long chain‑of‑thought reasoning into compact models, enabling effective short‑context reasoning across diverse tasks and modalities."
c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb,"Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.",Training Language Models to Self-Correct via Reinforcement Learning,"Developing a multi‑turn online reinforcement‑learning framework that trains a language model to self‑correct using only its own generated correction traces. The method first initializes a policy through multi‑turn RL to avoid behavior collapse, then applies regularized reward shaping—including a self‑correction bonus—to steer learning toward effective correction behavior under the model’s own distribution, eliminating the need for external supervision or multiple models."
182c7b40ff7560a5545764814338f55a2098e441,"Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.",Reinforced Self-Training (ReST) for Language Modeling,"Generating samples from an initial language model and then applying offline reinforcement learning to those samples in order to iteratively improve the model’s policy. This “reinforced self‑training” loop treats the generated data as a reusable batch, enabling efficient alignment of the model with human preferences without requiring online interaction during training."
9a75e23639bfcc3a51da57a3b682a984d1d8ac0b,"Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.",Language Models can Solve Computer Tasks,"Generating and refining actions by having a pre‑trained large language model recursively critique its own output (RCI) to solve computer tasks from natural language instructions. This prompting scheme iteratively evaluates and improves the generated code or command sequences, enabling task execution with minimal demonstrations and without task‑specific reward functions, and can be combined with chain‑of‑thought reasoning to further enhance performance."
1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0,"We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6% (8.6% improvement beyond format correction), and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7% (7.0% non-format gain). This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which contains the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8%, average: 36.6%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples. In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-category generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the""grokking""phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. All resources are open source at https://github.com/ypwang61/One-Shot-RLVR.",Reinforcement Learning for Reasoning in Large Language Models with One Training Example,"Leveraging reinforcement learning with verifiable reward (RLVR) by training on a single carefully selected example, and optimizing the policy via a gradient‑based loss augmented with entropy regularization to promote exploration, to enhance the mathematical reasoning capabilities of large language models. This approach isolates the effect of the policy‑gradient component, demonstrating that even minimal supervised signals can drive substantial improvements in reasoning performance across diverse models and benchmarks."
900cd128482bbab4d2752d01ce80c55498b78dd2,"The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution,"Leveraging reinforcement learning with a lightweight rule‑based reward (e.g., similarity between ground‑truth and generated solutions), SWE‑RL trains large language models on massive software‑evolution datasets that capture a project's full lifecycle—code snapshots, changes, issues, and pull requests. By learning to maximize this reward, the model autonomously reconstructs developers’ reasoning processes and solution strategies, thereby scaling LLM reasoning capabilities to real‑world software engineering tasks."
bde841b0dbbf7a15ee69966a828c7fe2cf532ad9,"Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.",WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning,"Generating a self‑evolving curriculum that creates new web interaction tasks from failed attempts, and training open‑source LLMs with an outcome‑supervised reward model combined with adaptive reinforcement‑learning updates, to continuously improve their decision‑making and execution capabilities on web‑based tasks."
69535d3c6ff3238f8e7b2b29c1d40ea9d9d7914f,"Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing large language model (LLM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LLMs. Through empirical analysis, we uncover positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LLMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LLM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LLM reasoning.",Reasoning with Exploration: An Entropy Perspective,"Augmenting the reinforcement‑learning advantage function with an entropy‑based term that rewards high‑entropy regions of the language model’s token distribution. By leveraging the observed correlation between entropy and exploratory reasoning actions—such as pivotal logical tokens, self‑verification steps, and rare behaviors—the method promotes longer and deeper reasoning chains rather than mere uncertainty, thereby integrating exploration signals directly into the RL optimization of LLM reasoning."
10feab31bb9e71a0f1094fb00c0554abfb992c4d,"Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models,"Prolonging reinforcement learning (ProRL) training by integrating KL‑divergence regularization, periodic resetting of the reference policy, and a broad, multi‑task curriculum. This methodology enables language models to iteratively explore and populate previously unreachable regions of the solution space, thereby acquiring novel reasoning strategies that are not present in the base model’s distribution."
8e3b1f5d8b6c165f64137cc1f7dea89cf6f622bd,"Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released at https://dllm-reasoning.github.io/.",d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning,"Adapting pre‑trained masked diffusion language models for reasoning by jointly applying masked supervised finetuning and a novel critic‑free policy‑gradient reinforcement learning algorithm. The approach distills self‑improvement behavior from existing datasets via masked SFT and integrates the diffu‑GRPO algorithm to directly optimize reasoning performance in a coarse‑to‑fine, non‑autoregressive generation paradigm."
4aef44e4aeaf28868ae2f1fff2c4eb19ff4df1f6,"Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.",The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning,"Minimizing the entropy of language‑model predictions to concentrate probability mass on the model’s most confident tokens. This is achieved through three techniques: (1) fine‑tuning the model on its own unlabeled outputs with a token‑level entropy loss, (2) applying reinforcement learning where the sole reward is negative entropy, and (3) adjusting logits at inference time to reduce entropy without any parameter updates. By directly encouraging low‑entropy outputs, the approach elicits latent reasoning abilities of pretrained LLMs without requiring labeled data."
c78350e81298ca87bc1d59b466fa40081232caaa,"Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.",Teaching Large Language Models to Reason with Reinforcement Learning,"Evaluating and comparing a suite of reinforcement‑learning‑from‑feedback algorithms—including Expert Iteration, Proximal Policy Optimization, and Return‑Conditioned RL—by training large language models with both sparse and dense reward signals derived from heuristic functions or learned reward models, across various model scales and initialization conditions (with and without supervised fine‑tuning). The study systematically measures how these algorithms affect reasoning performance and explores the interaction between supervised fine‑tuning and subsequent RL fine‑tuning."
8d6411e337502f7fe0bfa59d486803a73d2c1192,"Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks. However, existing approaches mainly rely on imitation learning and struggle to achieve effective test-time scaling. While reinforcement learning (RL) holds promise for enabling self-exploration, recent attempts yield modest improvements in complex reasoning. In this paper, we present T1 to scale RL by encouraging exploration and understand inference scaling. We first initialize the LLM using synthesized chain-of-thought data that integrates trial-and-error and self-verification. To scale RL training, we promote increased sampling diversity through oversampling. We demonstrate that T1 with open LLMs as its base exhibits inference scaling behavior and achieves superior performance on challenging math reasoning benchmarks. More importantly, we present a simple strategy to examine inference scaling, where increased inference budgets directly lead to T1's better performance without any additional verification.",Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling,"Scaling reinforcement learning for large language models by generating synthetic chain‑of‑thought data that combines trial‑and‑error exploration with self‑verification, and by increasing sampling diversity through oversampling during training. This approach enables the model to exhibit inference scaling, where larger inference budgets directly improve performance without additional verification steps."
529ff7d6441d244212cf2becafd12a7e67ac56d9,"Large language models (LLMs) have demonstrated great capabilities in various natural language understanding and generation tasks. These pre-trained LLMs can be further improved for specific downstream tasks by fine-tuning. However, the adoption of LLM in real-world applications can be hindered by privacy concerns and the resource-intensive nature of model training and fine-tuning. When multiple entities have similar interested tasks but cannot directly share their local data due to privacy regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. Besides avoiding direct data sharing, FL can also achieve rigorous data privacy protection, model intelligent property protection, and model customization via composition with different techniques. Despite the aforementioned advantages of FL, fine-tuning LLMs in FL settings still lacks adequate support from the existing frameworks and, therefore, faces challenges in optimizing the consumption of significant communication and computational resources, preparing various data for different tasks, and satisfying diverse information protection demands. In this paper, we discuss these challenges and introduce our package FederatedScope-LLM (FS-LLM) as a main contribution, which consists: (1) We build a complete end-to-end benchmarking pipeline under real-world scenarios, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation; (2) We provide comprehensive and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations and versatile programming interfaces for future extension, enhancing the capabilities of LLMs in FL scenarios with low communication and computation costs, even without accessing the full model; (3) We adopt several accelerating and resource-efficient operators, and provide flexible pluggable sub-routines for interdisciplinary study. We conduct extensive and reproducible experiments to show the effectiveness of FS-LLM and benchmark advanced LLMs with PEFT algorithms in FL. We release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.",FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,"Enabling federated fine‑tuning of large language models by constructing a comprehensive, end‑to‑end pipeline that automates dataset preprocessing, federated execution, and evaluation, while integrating off‑the‑shelf parameter‑efficient fine‑tuning (PEFT) algorithms. Incorporating resource‑efficient operators and modular, pluggable sub‑routines reduces communication and computational overhead, allowing LLMs to be adapted across distributed, privacy‑sensitive environments without requiring access to the full model."
04c05c6acc970f2ca89af8e436b8dd8189396146,"Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the""Zero""reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.",General-Reasoner: Advancing LLM Reasoning Across All Domains,"Constructing a large-scale, high‑quality dataset of multi‑domain questions with verifiable answers through extensive web crawling, and developing a generative model‑based answer verifier that leverages chain‑of‑thought reasoning and context awareness to replace traditional rule‑based verification. Training LLMs with this dataset under a “Zero” reinforcement learning paradigm enables direct reinforcement learning fine‑tuning of base models, thereby enhancing their reasoning capabilities across diverse fields without relying on intermediate supervised stages."
2d906cda427cb2c4a71069423312e57ba4cd5445,"Reinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Despite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements. In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements. Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",Reinforcement Learning Enhanced LLMs: A Survey,"Conducting a systematic review of reinforcement‑learning‑enhanced large language models by organizing the foundational concepts of RL, cataloguing prominent RL‑augmented LLM architectures, and synthesizing the major reward‑model‑based techniques such as RL from Human Feedback (RLHF), RL from AI Feedback (RLAIF), and Direct Preference Optimization (DPO). This approach consolidates current research, highlights methodological challenges, and outlines prospective directions for advancing RL‑driven LLM alignment without presenting empirical results."
668858489bbec3ce45f7a84a6a557b329f9ec91a,"A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or""agent""tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).",ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL,"Developing a hierarchical reinforcement‑learning framework that simultaneously runs a high‑level off‑policy value‑based RL algorithm to aggregate rewards across utterances and a low‑level RL algorithm that leverages this value function to train a token‑level policy within each turn. This approach extends existing single‑turn RL methods for LLMs to handle multi‑turn interactions, long horizons, and delayed rewards while preserving their flexibility and enabling efficient fine‑tuning of large language models for agent tasks."
6c00f661c46391642208a5292f38b5a9e0e09cae,"Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark---AutoWebBench---for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at https://github.com/THUDM/AutoWebGLM.",AutoWebGLM: A Large Language Model-based Web Navigating Agent,"Simplifying HTML pages with a custom algorithm that preserves essential information to create concise webpage representations, and then training a large language model using a hybrid human‑AI generated curriculum of browsing data. Bootstrapping the model further with reinforcement learning and rejection sampling enables it to improve webpage comprehension, execute browser actions, and autonomously decompose complex navigation tasks."
53106a642a12b05753ebe9ffca62d8efb0670281,"Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model is fine-tuned in a manner that directly optimizes the performance of the inference-time strategy. We study this paradigm using the simple yet effective Best-of-N (BoN) inference strategy, in which a verifier selects the best out of a set of LLM-generated responses. We devise the first imitation learning and reinforcement learning~(RL) methods for BoN-aware fine-tuning, overcoming the challenging, non-differentiable argmax operator within BoN. We empirically demonstrate that our BoN-aware models implicitly learn a meta-strategy that interleaves best responses with more diverse responses that might be better suited to a test-time input -- a process reminiscent of the exploration-exploitation trade-off in RL. Our experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, we show that our methods improve the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6% to 67.1%.",Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models,"Optimizing large language models for inference-time strategies by fine‑tuning them in an inference‑aware manner. This is achieved by formulating Best‑of‑N (BoN) fine‑tuning as an imitation‑learning and reinforcement‑learning problem that circumvents the non‑differentiable argmax selection, enabling the model to learn a meta‑strategy that balances generating high‑quality responses with diverse alternatives for downstream verification."
0a42291d9543eabe33f6c14278333484071a707c,"Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline Reasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost performance during test time.",Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Jointly learning a policy model and a value function by optimizing the soft Bellman equation to perform offline reinforcement learning for multi‑step reasoning in large language models, thereby reducing the dependence on paired preference data and improving credit assignment across reasoning steps. The learned value function can also be used to guide test‑time tree search, further enhancing reasoning performance without additional training."
95d638e7705ec561382268405bc488df4c26c7f7,"Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1.",Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning,"Leveraging a two‑stage architecture that first distills a financial‑reasoning dataset from DeepSeek‑R1 and then applies supervised fine‑tuning followed by reinforcement learning, the method adapts a 7 billion‑parameter language model for the financial domain. By integrating domain‑specific data distillation with sequential SFT and RL training, the approach endows the model with enhanced reasoning and decision‑making capabilities tailored to complex financial tasks."
4f0e4a313a3f777b4b6aab4f364b9bc51a6aacc9,"The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.",Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging,"Merging complementary language models to enable Long-to-Short (L2S) reasoning. By integrating a fast, System 1 model with a deliberative, System 2 model through techniques such as task‑vector alignment, singular‑value‑decomposition (SVD) based blending, and activation‑informed weight interpolation, the approach creates a single model that can dynamically adjust reasoning depth, producing concise yet robust responses without the computational overhead of separate fine‑tuning or reinforcement‑learning pipelines."
aa89c6bf86486e180833037333555e3492b15c8e,"Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs'performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.",MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning,"Leveraging multi-agent reinforcement learning to co‑train multiple large language models (LLMs) for collaborative problem solving. The approach lets each LLM generate an initial response, then engage in a multi‑turn discussion to refine the answer, while a dedicated verifier scores both the final answer and the discussion, providing a reward signal that encourages corrective and persuasive interactions; this reward is optimized jointly across the agents to induce coordinated collaborative behavior."
24d9d00b91f99dde3c9a0ea5c79e63f2ed26151c,"The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree.",Reinforcing General Reasoning without Verifiers,"Bypassing explicit answer verification and directly maximizing the probability of generating reference answers through reinforcement learning, VeriFree trains a single language model that implicitly learns to assess its own outputs. This approach integrates policy learning and an implicit verifier within a unified model, framed as a variational optimization problem, thereby eliminating the need for a separate verifier model during training."
1f45c9d4d92c51a94c984eb4c2c6e027bbf78038,"Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, offering valuable insights into scaling LLM reasoning capabilities across diverse tasks.",SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM,"Introducing a two‑stage cross‑domain training paradigm that first cultivates mathematical reasoning and then coding proficiency, while integrating History Resampling (HR) to recycle ineffective samples during policy learning. Building on Group Relative Policy Optimization (GRPO), this approach jointly optimizes the policy across domains by resampling and reweighting past trajectories, thereby enhancing the efficiency and generality of reinforcement‑learning‑based reasoning for large language models."
82f59319e581cdfe16a97fe29bec6215ad818a81,"Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.",Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions,"Interleaving reinforcement learning with online supervised fine‑tuning to enable large language models to acquire new knowledge and reasoning patterns beyond their original capabilities. The approach alternates between RL updates on existing tasks and targeted SFT on challenging examples, using high‑quality demonstrations collected on‑the‑fly to continuously expand the model’s reasoning repertoire."
765051eee0fb1394b62555edb86ba1f7a00892fb,"Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is:{last round answer}, and please re-answer.",Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking,"Implementing test-time scaling through Multi‑round Thinking, which iteratively refines a language model’s reasoning by feeding its prior answer back as a prompt for subsequent inference rounds. This simple prompting scheme enables the model to progressively improve its responses without additional training, offering a broadly applicable method for enhancing performance on complex tasks."
59084df7203c6be33838ba3e3854eb9bda053ed2,"Reinforcement learning (RL) has been widely used in training large language models (LLMs) for preventing unexpected outputs, eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at https://github.com/RUCAIBox/RLMEC.",Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint,"Incorporating a generative model as a token‑level reward function that is trained on an erroneous‑solution rewriting task under a minimum‑editing constraint, and using this reward to formulate a token‑wise reinforcement‑learning objective augmented with an imitation‑based regularization. This approach directs the RL optimization toward the crucial tokens that cause errors, while attenuating the influence of irrelevant tokens during training."
d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855,"Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large Language Models (LLMs), typically paired with the Proximal Policy Optimization (PPO) algorithm. While PPO is a powerful method designed for general reinforcement learning tasks, it is overly sophisticated for LLMs, leading to laborious hyper-parameter tuning and significant computation burdens. To make RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. These properties are not exploited in PPO, making it less suitable for RLHF. Building on the renowned REINFORCE algorithm, ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique. ReMax offers several benefits over PPO: it is simpler to implement, eliminates more than 4 hyper-parameters in PPO, reduces GPU memory usage, and shortens training time. ReMax can save about 46% GPU memory than PPO when training a 7B model and enables training on A800-80GB GPUs without the memory-saving offloading technique needed by PPO. Applying ReMax to a Mistral-7B model resulted in a 94.78% win rate on the AlpacaEval leaderboard and a 7.739 score on MT-bench, setting a new SOTA for open-source 7B models. These results show the effectiveness of ReMax while addressing the limitations of PPO in LLMs.","ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models","Leveraging the fast, deterministic simulation and trajectory‑level reward structure of RLHF, ReMax replaces PPO with a REINFORCE‑based algorithm that omits the auxiliary value model and reduces the hyper‑parameter set. By introducing a new variance‑reduction technique tailored to these RLHF properties, the method simplifies implementation, cuts GPU memory usage, and accelerates training while preserving alignment performance."
3646acd0dc49a00d38517abccfc3a54cb78bbadc,"Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.",SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning,"Generating a reflection-focused training set with an advanced multimodal LLM and then applying a two‑stage reinforcement‑learning framework to teach a policy model both reasoning and self‑reflection. The first stage uses the generated reflections to fine‑tune the model, while the second stage employs Group Relative Policy Optimization with a reward that promotes concise, cognitively meaningful reflections and penalizes redundancy, thereby enhancing multimodal reasoning capabilities."
2bdccbeffa0bae760f416a72ebe7a3951e230659,"Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360",Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective,"Curating a large-scale, multi-domain reinforcement‑learning (RL) reasoning corpus (Guru) that provides reliable, verifiable reward signals for six distinct reasoning areas, and using this corpus to train LLMs with domain‑specific reward design, deduplication, and filtering. Leveraging the curated data to systematically apply RL fine‑tuning across and within domains, enabling the models to acquire reasoning skills beyond pretraining knowledge through in‑domain and cross‑domain training."
c226a4acb42912054d498bcf771023b0ba2da001,"Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.",Language Model Self-improvement by Reinforcement Learning Contemplation,"Generating answers to unlabeled inputs and simultaneously using the same language model to evaluate those answers, then updating the model parameters with reinforcement learning to maximize the self‑assigned quality scores. This dual student‑teacher framework enables unsupervised self‑improvement of large language models across various NLP tasks without external supervision."
7d6168fbd3ed72f9098573007f4b8c2ec9e576b9,"In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning,"Developing a reverse‑curriculum reinforcement learning framework that transforms outcome‑only supervision into effective step‑wise guidance for large language models. By progressively moving the initial state of a reasoning episode backward from the demonstration’s end toward its beginning, the method creates a curriculum that lets the model explore easier sub‑tasks first, enabling sparse final‑result rewards to be decomposed into informative intermediate signals and precise error localization without any manual process annotations."
8db1d30ac06dde4bd19d0a86241137ac2be21552,"In software development, the predominant emphasis on functionality often supersedes security concerns, a trend gaining momentum with AI-driven automation tools like GitHub Copilot. These tools significantly improve developers' efficiency in functional code development. Nevertheless, it remains a notable concern that such tools are also responsible for creating insecure code, predominantly because of pre-training on publicly available repositories with vulnerable code. Moreover, developers are called the""weakest link in the chain""since they have very minimal knowledge of code security. Although existing solutions provide a reasonable solution to vulnerable code, they must adequately describe and educate the developers on code security to ensure that the security issues are not repeated. Therefore we introduce a multipurpose code vulnerability analysis system \texttt{SecRepair}, powered by a large language model, CodeGen2 assisting the developer in identifying and generating fixed code along with a complete description of the vulnerability with a code comment. Our innovative methodology uses a reinforcement learning paradigm to generate code comments augmented by a semantic reward mechanism. Inspired by how humans fix code issues, we propose an instruction-based dataset suitable for vulnerability analysis with LLMs. We further identify zero-day and N-day vulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings underscore that incorporating reinforcement learning coupled with semantic reward augments our model's performance, thereby fortifying its capacity to address code vulnerabilities with improved efficacy.",LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward,"Generating vulnerability analyses and fixes by training a large language model with reinforcement learning that incorporates a semantic reward for producing accurate code comments and remediation suggestions. The approach builds an instruction‑based dataset for vulnerability analysis, enabling the model to identify security issues, generate descriptive comments, and synthesize corrected code in a single, developer‑assistive workflow."
3714ed902e79dad5dcc93c5d033c8222d044f3c8,"Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells — up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at this link: https://doi.org/10.6084/m9.figshare.25983166.",Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,"Leveraging reinforcement learning with static analysis–based quality metrics as a reward model to fine‑tune large language models for unit test generation. By defining lightweight static analysis metrics that capture test code quality and using them to guide the RL optimization of a pre‑trained LLM, the approach directly incentivizes the model to produce syntactically correct tests that adhere to best practices and avoid test smells."
0fe8f2f55046ad0b8d6337f57a78466790923264,"Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.",Outcome-based Exploration for LLM Reasoning,"Introducing outcome-based exploration for reinforcement‑learning fine‑tuning of large language models, which assigns exploration bonuses based on the rarity of final answers. The approach implements two complementary mechanisms—historical exploration that rewards rarely observed outcomes via UCB‑style bonuses, and batch exploration that penalizes repeated answers within a generation batch—to maintain generation diversity while still optimizing for answer correctness."
